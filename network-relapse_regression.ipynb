{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "#device = 0\n",
    "#torch.cuda.set_device(device)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('../fastai/')\n",
    "import fastai\n",
    "from fastai.vision import *\n",
    "from fastai.vision.learner import model_meta\n",
    "\n",
    "sys.path.append('../models-pytorch/pretrained-models.pytorch')\n",
    "import pretrainedmodels\n",
    "from pretrainedmodels import *\n",
    "\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models import *\n",
    "from torchsummary import summary\n",
    "from pathlib import Path\n",
    "from functools import partial, update_wrapper\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.image as mpimg\n",
    "import shutil\n",
    "\n",
    "\n",
    "PATH = Path('/home/Deep_Learner/private/network/datasets/Hypophysenadenome-Rezidive/')\n",
    "PATH_LOCAL = Path('/home/Deep_Learner/private/local/')\n",
    "WSIS_RELAPSE = PATH/'wsis_relapse'\n",
    "TILES_RELAPSE = PATH_LOCAL/'tiles_relapse'\n",
    "\n",
    "LABELS_NAME = 'rezidive-labels.xlsx'\n",
    "LABELS = PATH/LABELS_NAME\n",
    "\n",
    "\n",
    "\n",
    "nw = 16   #number of workers for data loader\n",
    "torch.backends.cudnn.benchmark=True\n",
    "\n",
    "#def batch_stats(self, funcs:Collection[Callable]=None)->Tensor:\n",
    "#        \"Grab a batch of data and call reduction function `func` per channel\"\n",
    "#        funcs = ifnone(funcs, [torch.mean,torch.std])\n",
    "#        x = self.one_batch(ds_type=DatasetType.Train, denorm=False)[0].cpu()\n",
    "#        return [func(channel_view(x), 1) for func in funcs]\n",
    "#        \n",
    "#vision.data.ImageDataBunch.batch_stats = batch_stats\n",
    "\n",
    "sz = 512\n",
    "bs = 6\n",
    "\n",
    "#fastai defaults\n",
    "tta_beta = 0.4 \n",
    "tta_scale = 1.35\n",
    "dropout = 0.5\n",
    "wd = 0.01\n",
    "\n",
    "seed = 69\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.torch_core import flatten_model\n",
    "\n",
    "def arch_summary(arch):\n",
    "    model = arch(False)\n",
    "    tot = 0\n",
    "    for i, l in enumerate(model.children()):\n",
    "        n_layers = len(flatten_model(l))\n",
    "        tot += n_layers\n",
    "        print(f'({i}) {l.__class__.__name__:<12}: {n_layers:<4}layers (total: {tot})')\n",
    "\n",
    "def show(np):\n",
    "    return util.np_to_pil(np)\n",
    "\n",
    "Path.ls = lambda x: [p for p in list(x.iterdir()) if '.ipynb_checkpoints' not in p.name]\n",
    "\n",
    "def show_multiple_images(path, rows = 3, figsize=(128, 64)):\n",
    "    imgs = [open_image(p) for p in path.ls()]\n",
    "    show_all(imgs=imgs, r=rows, figsize=figsize)\n",
    "    \n",
    "def show_multiple_images_big(path:pathlib.Path):\n",
    "    for p in path.ls():\n",
    "        plt.imshow(mpimg.imread(str(p)))\n",
    "        plt.show()\n",
    "        \n",
    "def get_id_from_path(path):\n",
    "    path = Path(path)\n",
    "    split = path.stem.split('-')\n",
    "    return f'{split[0]}-{split[1]}'\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    result = []\n",
    "    for l in list_of_lists:\n",
    "        if len(l) == 1:\n",
    "            result.append(l[0])\n",
    "        else:\n",
    "            for elem in l:\n",
    "                result.append(elem)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/PPPW/deep-learning-random-explore/blob/master/CNN_archs/cnn_archs.ipynb\n",
    "\n",
    "def identity(x): return x\n",
    "\n",
    "def nasnetamobile(pretrained=True):\n",
    "    pretrained = 'imagenet' if pretrained else None\n",
    "    model = pretrainedmodels.nasnetamobile(pretrained=pretrained, num_classes=1000)  \n",
    "    model.logits = identity\n",
    "    model_meta[nasnetamobile] =  { 'cut': identity, 'split': lambda m: (list(m[0][0].children())[8], m[1]) }\n",
    "    return nn.Sequential(model)\n",
    "\n",
    "#arch_summary(lambda _: nasnetamobile(False)[0])\n",
    "\n",
    "def se_resnext50_32x4d(pretrained=True):\n",
    "    pretrained = 'imagenet' if pretrained else None\n",
    "    model = pretrainedmodels.se_resnext50_32x4d(pretrained=pretrained)\n",
    "    model_meta[se_resnext50_32x4d] =  {'cut': -2, 'split': lambda m: (m[0][3], m[1]) }\n",
    "    return model\n",
    "\n",
    "#arch_summary(lambda _: pretrainedmodels.se_resnext50_32x4d(pretrained=None))\n",
    "\n",
    "def se_resnext101_32x4d(pretrained=True):\n",
    "    pretrained = 'imagenet' if pretrained else None\n",
    "    model = pretrainedmodels.se_resnext101_32x4d(pretrained=pretrained)\n",
    "    model_meta[se_resnext101_32x4d] =  {'cut': -2, 'split': lambda m: (m[0][3], m[1]) }\n",
    "    return model\n",
    "\n",
    "def xception(pretrained=True):\n",
    "    pretrained = 'imagenet' if pretrained else None\n",
    "    model = pretrainedmodels.xception(pretrained=pretrained)\n",
    "    model_meta[xception] =  { 'cut': -1, 'split': lambda m: (m[0][11], m[1]) }\n",
    "    return model\n",
    "\n",
    "def inceptionv4(pretrained=True):\n",
    "    pretrained = 'imagenet' if pretrained else None\n",
    "    model = pretrainedmodels.inceptionv4(pretrained=pretrained)\n",
    "    model_meta[xception] =  { 'cut': -2, 'split': lambda m: (m[0][11], m[1]) }\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "n = np.load('n-rez-reg.npy')\n",
    "print(n)\n",
    "m = n+1\n",
    "m=1\n",
    "np.save('n-rez-reg', m)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split dataset into train, valid and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 69\n",
      "32128\n",
      "36\n",
      "3797\n",
      "7\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(LABELS).set_index('id')\n",
    "test_pct = 0\n",
    "valid_pct = 0.15\n",
    "\n",
    "#key: patient, value: list of wsi names\n",
    "patient_to_wsi_ids_relapse = {}\n",
    "ids_relapse_all = [get_id_from_path(p) for p in (WSIS_RELAPSE.ls()) if p.suffix == '.ndpi']\n",
    "excluded_ids = []\n",
    "for id in ids_relapse_all:\n",
    "    if id not in excluded_ids:\n",
    "        patient = df.at[id, 'Patient']\n",
    "        if patient in patient_to_wsi_ids_relapse.keys():\n",
    "            patient_to_wsi_ids_relapse[patient].append(id)\n",
    "        else:\n",
    "            patient_to_wsi_ids_relapse[patient] = [id]\n",
    "if test_pct != 0:            \n",
    "    patients_relapse_train_and_valid, patients_relapse_test = train_test_split(list(patient_to_wsi_ids_relapse.keys()), \n",
    "                                                                               test_size=test_pct, \n",
    "                                                                               random_state=seed)\n",
    "    patients_relapse_train, patients_relapse_valid = train_test_split(patients_relapse_train_and_valid, \n",
    "                                                                      test_size=valid_pct, \n",
    "                                                                      random_state=seed)\n",
    "else:\n",
    "    patients_relapse_train, patients_relapse_valid = train_test_split(list(patient_to_wsi_ids_relapse.keys()), \n",
    "                                                                      test_size=valid_pct, \n",
    "                                                                      random_state=seed)\n",
    "    patients_relapse_test = []\n",
    "    \n",
    "    \n",
    "\n",
    "ids_relapse_train = flatten([patient_to_wsi_ids_relapse[pat] for pat in patients_relapse_train])\n",
    "ids_relapse_valid = flatten([patient_to_wsi_ids_relapse[pat] for pat in patients_relapse_valid])\n",
    "ids_relapse_test = flatten([patient_to_wsi_ids_relapse[pat] for pat in patients_relapse_test])\n",
    "\n",
    "tile_paths_relapse_all = [Path(f'{p.parts[-2]}/{p.parts[-1]}') for p in (TILES_RELAPSE.ls()) if p.suffix == '.png']\n",
    "tile_paths_train = [p for p in tile_paths_relapse_all if get_id_from_path(p) in ids_relapse_train]\n",
    "tile_paths_val = [p for p in tile_paths_relapse_all if get_id_from_path(p) in ids_relapse_valid]\n",
    "tile_paths_test = [p for p in tile_paths_relapse_all if get_id_from_path(p) in ids_relapse_test]\n",
    "\n",
    "df_tile_paths_train_and_valid = pd.DataFrame((tile_paths_train+tile_paths_val), columns=['name'])\n",
    "\n",
    "print(f'seed: {seed}')\n",
    "print(len(tile_paths_train))\n",
    "print(len(set([get_id_from_path(p) for p in tile_paths_train])))\n",
    "print(len(tile_paths_val))\n",
    "print(len(set([get_id_from_path(p) for p in tile_paths_val])))\n",
    "print(len(tile_paths_test))\n",
    "print(len(set([get_id_from_path(p) for p in tile_paths_test])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = get_transforms(flip_vert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfms = ([RandTransform(tfm=TfmAffine (dihedral_affine), kwargs={}, p=1.0, resolved={}, do_run=True, is_random=True),\n",
    "#        RandTransform(tfm=TfmLighting (brightness), kwargs={'change': (0.475, 0.525)}, p=0.75, resolved={}, do_run=True, is_random=True),\n",
    "#        RandTransform(tfm=TfmLighting (contrast), kwargs={'scale': (0.95, 1.0526315789473684)}, p=0.75, resolved={}, do_run=True, is_random=True)],\n",
    "#        [])\n",
    "\n",
    "#def get_ex(): return open_image(str(TRAIN.ls()[0]))\n",
    "#\n",
    "#def plots_f(rows, cols, width, height, **kwargs):\n",
    "#    [get_ex().apply_tfms(tfms[0], **kwargs).show(ax=ax) for i,ax in enumerate(plt.subplots(\n",
    "#        rows,cols,figsize=(width,height))[1].flatten())]\n",
    "#\n",
    "#plots_f(2, 4, 12, 6, size=224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datablock API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_func(path):    \n",
    "    return df.loc[get_id_from_path(Path(path))]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_func(path):\n",
    "    return get_id_from_path(Path(path)) in (ids_relapse_valid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = ImageList.from_folder(path=TRAIN, extensions=['.png'])\n",
    "data = ImageList.from_df(df_tile_paths_train_and_valid, path=PATH_LOCAL)\n",
    "data = data.split_by_valid_func(split_func)\n",
    "data = data.label_from_func(label_func)\n",
    "data = data.transform(tfms=tfms, size=sz)\n",
    "#data = data.add_test_folder(test_folder=TEST_EXPERIMENTING)\n",
    "if test_pct != 0:\n",
    "    data = data.add_test([PATH/p for p in tile_paths_test])\n",
    "data = data.databunch(bs=bs, num_workers=nw, path=PATH/f'{n}-regression-currently-training')\n",
    "data = data.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlattenedLoss of MSELoss()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loss_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_frozen = 5\n",
    "epochs_unfrozen = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = resnext101_32x8d\n",
    "learner = cnn_learner(data=data, \n",
    "                     base_arch=arch, \n",
    "                     metrics=[mean_squared_error], \n",
    "                     ps=dropout, \n",
    "                     pretrained=True, \n",
    "                     wd = wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1-regression-resnext101_32x8d-size512-bs6-epochs_head5-epochs_complete10-seed_69-test_pct_0-valid_pct_0.15'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nameBase = f'{n}-regression-{arch.__name__}-size{sz}-bs{bs}-epochs_head{epochs_frozen}-epochs_complete{epochs_unfrozen}-seed_{seed}-test_pct_{test_pct}-valid_pct_{valid_pct}'\n",
    "nameBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.lr_find()\n",
    "learner.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='5', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/5 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='97' class='' max='5354', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      1.81% [97/5354 00:41<37:48 1965834.7500]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.fit_one_cycle(cyc_len=epochs_frozen, max_lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameHead = f'{nameBase}-head'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save(nameHead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner.load('bestmodel_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learner.lr_find(start_lr=1e-10, end_lr=10, num_it=1000)\n",
    "learner.recorder.plot(skip_start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = 1e-6\n",
    "lr3 = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(cyc_len=epochs_unfrozen, \n",
    "                      max_lr=slice(lr2, lr3), \n",
    "                      callbacks=[SaveModelCallback(learner, every='epoch', monitor='mean_squared_error')], \n",
    "                      start_epoch=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.recorder.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameComplete = f'{nameBase}-complete'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save(nameComplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner.load(nameComplete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction per case  !!TODO!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(dict_arch_to_path_of_saved_model:typing.Dict[Callable, pathlib.Path], \n",
    "                     data:fastai.vision.data.ImageDataBunch,\n",
    "                     ds_type:fastai.basic_data.DatasetType,\n",
    "                     tta:bool, \n",
    "                     scale:float,\n",
    "                     beta:float):\n",
    "    \"\"\"\n",
    "    tta: Should test time augmentation be used?\n",
    "    scale: if tta is True -> scaling factor for tta\n",
    "    beta: if tta is True -> beta factor for tta\n",
    "    check this out for more infos: https://docs.fast.ai/basic_train.html#Test-time-augmentation\n",
    "    \"\"\"\n",
    "   \n",
    "    print(f'{str([a.__name__ for a in dict_arch_to_path_of_saved_model.keys()])}_sz{sz}_ensembled')\n",
    "    \n",
    "    predsList = []\n",
    "    for arch in dict_arch_to_path_of_saved_model.keys():\n",
    "        learner = cnn_learner(data=data, base_arch=arch, pretrained=False)\n",
    "        learner.load(dict_arch_to_path_of_saved_model[arch])\n",
    "        if tta is True:\n",
    "            preds = learner.TTA(beta=beta, scale=scale, ds_type=ds_type)\n",
    "        else:\n",
    "            preds = learner.get_preds(ds_type=ds_type)\n",
    "            \n",
    "        predsList.append(preds)\n",
    "    \n",
    "    preds_ensembled = predsList[0]\n",
    "    for n, _ in enumerate(predsList):\n",
    "        if n == 0:\n",
    "            continue\n",
    "        else:\n",
    "            preds_ensembled[0] = preds_ensembled[0] + predsList[n][0]\n",
    "    preds_ensembled[0] = preds_ensembled[0]/len(predsList)\n",
    "    \n",
    "    return preds_ensembled\n",
    "\n",
    "def from_preds_to_dict_path_to_preds(preds, \n",
    "                                     imageDataBunch:fastai.vision.ImageDataBunch, \n",
    "                                     ds_type:fastai.basic_data.DatasetType,\n",
    "                                     threshold:float):\n",
    "    \"\"\"\n",
    "    preds: What fastai.vision.learner.get_preds or fastai.vision.learner.TTA return.\n",
    "            two tensors: 1st: tensors with raw predictions for each image\n",
    "                         2nd: tensors with y_true\n",
    "            form e.g. [tensor([[60.], \n",
    "                                [72.]]),\n",
    "                         tensor([[523.],\n",
    "                                [109.]])]\n",
    "                                \n",
    "    RETURN:\n",
    "        key:path, value:tensor with one float number (tensor with raw pred) \n",
    "        e.g. tensor([66.1246])\n",
    "    path_to_pred = {}\n",
    "    \"\"\"\n",
    "    #key:path, value:tensor with one float number (tensor with raw pred) \n",
    "    #e.g. tensor([66.1246])\n",
    "    path_to_pred = {}\n",
    "    d = None\n",
    "    if ds_type is DatasetType.Valid:\n",
    "        d = imageDataBunch.valid_ds\n",
    "    elif ds_type is DatasetType.Test:\n",
    "        d = imageDataBunch.test_ds\n",
    "    elif ds_type is DatasetType.Train:\n",
    "        d = imageDataBunch.train_ds\n",
    "    for path, pred in tqdm(zip(d.items, preds[0]), total = len(d.items)):\n",
    "        path_to_pred[path] = pred\n",
    "        \n",
    "    return path_to_pred\n",
    "\n",
    "\n",
    "def get_mean_prediction_per_id(learner:fastai.vision.learner=None,\n",
    "                               labelList:fastai.data_block.LabelList=None,\n",
    "                               dict_arch_to_path_of_saved_model:typing.Dict[Callable, pathlib.Path]=None,\n",
    "                               imageDataBunch:fastai.vision.data.ImageDataBunch=None,\n",
    "                               ds_type:fastai.basic_data.DatasetType=None,\n",
    "                               tta:bool=False,                                          \n",
    "                               threshold = 0.5,                              \n",
    "                               scale:float = 1.35,\n",
    "                               beta: float = 0.4):\n",
    "    \"\"\"\n",
    "    Option 1: Hand over a fastai.vision.learner and fastai.data_block.LabelList. No tta and no ensembling available\n",
    "                for this option.\n",
    "    Option 2: Hand over a fastai.vision.learner that was initalized with a fastai.vision.data.ImageDataBunch object.\n",
    "    Option 3: Hand over dict where the keys are functions to create a model (e.g. torchvision.models.resnet50)\n",
    "                and the values are paths to saved weights. Do this to use ensembling.\n",
    "    \n",
    "    Params:\n",
    "        threshold:  threshold to consider the predictions to be correct or not\n",
    "        scale: only needed when tta is True; scale value for fastai's fastai.basic_train.Learner.TTA function\n",
    "        beta: only needed when tta is True; beta value for fastai's fastai.basic_train.Learner.TTA function\n",
    "    \"\"\"\n",
    "    \n",
    "    if labelList is not None and ds_type is not None:\n",
    "        raise ValueError('One of dataset or ds_type must be None')\n",
    "    if labelList is not None and tta is True:\n",
    "        raise ValueError('TTA is not available for a custom LabelList')\n",
    "                \n",
    "    #key:path, value:tensor with one float number (tensor with raw pred) \n",
    "    #e.g. tensor([66.1246])\n",
    "    path_to_pred = {}\n",
    "    \n",
    "    #Option 1\n",
    "    if learner is not None and labelList is not None:\n",
    "        for n, path in tqdm(enumerate(labelList.items), total=len(labelList.items)):\n",
    "            pred = learner.predict(labelList[n][0], thresh=threshold)\n",
    "            path_to_pred[path] = pred\n",
    "    \n",
    "    #Option 2\n",
    "    elif learner is not None and labelList is None and  not dict_arch_to_path_of_saved_model and imageDataBunch is None:\n",
    "        if tta is True:\n",
    "            preds = learner.TTA(beta=beta, scale=scale, ds_type=ds_type)\n",
    "        else:\n",
    "            preds = learner.get_preds(ds_type=ds_type)\n",
    "        path_to_pred = from_preds_to_dict_path_to_preds(preds, learner.data, ds_type, threshold)\n",
    "                \n",
    "    #Option 3\n",
    "    elif dict_arch_to_path_of_saved_model and imageDataBunch is not None:\n",
    "        preds = ensemble_predict(dict_arch_to_path_of_saved_model, imageDataBunch, ds_type, tta, scale, beta)\n",
    "        path_to_pred = from_preds_to_dict_path_to_preds(preds, imageDataBunch, ds_type, threshold)                \n",
    "               \n",
    "    #key: id of a case; value: list with this syntax  \n",
    "    #[<number of tiles>, \n",
    "    #[<summed up predictions over all tiles>, \n",
    "    #y_true]\n",
    "    summed_up_predictions_per_id = {}\n",
    "    for path, pred in path_to_pred.items():   \n",
    "        id = get_id_from_path(path)\n",
    "        if id in summed_up_predictions_per_id:\n",
    "            v = summed_up_predictions_per_id[id]\n",
    "            v[0] = v[0] + 1\n",
    "            v[1] = v[1] + pred\n",
    "            summed_up_predictions_per_id[id] = v\n",
    "        else:\n",
    "            summed_up_predictions_per_id[id] = [1, pred, label_func(path)]\n",
    "            \n",
    "    #key: id of a case; value: list with this syntax  \n",
    "    #[<number of tiles>, \n",
    "    #[<mean prediction over all tiles>, \n",
    "    #y_true]\n",
    "    mean_prediction_per_id = {}\n",
    "    for key, value in summed_up_predictions_per_id.items():\n",
    "        mean_prediction_per_id[key] = [value[0], value[1]/value[0], value[1]]\n",
    "                \n",
    "    return mean_prediction_per_id\n",
    "\n",
    "\n",
    "def get_mean_absolute_error_over_all_ids(mean_prediction_per_id:dict):\n",
    "    summed_up_absolute_errors = 0\n",
    "    for k, v in mean_prediction_per_id.items():\n",
    "        summed_up_absolute_errors = summed_up_absolute_errors + abs(v[1] - v[2])\n",
    "    return float(summed_up_absolute_errors/len(mean_prediction_per_id.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arches = {resnext101_32x8d:Path(MODEL_PATH/'6-resnext101_32x8d-size512-bs8-seed_73/bestmodel_15'),\n",
    "         # se_resnext101_32x4d:MODEL_PATH/'11-se_resnext101_32x4d-size512-bs10-epochs_head5-epochs_complete5-seed_73/11-se_resnext101_32x4d-size512-bs8-epochs_head5-epochs_complete5-seed_73-complete'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mppi_val = get_mean_prediction_per_id(learner=learner, ds_type=DatasetType.Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mppi_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mean_absolute_error_over_all_ids(mppi_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mppi_test = get_class_occurence_per_id(learner=learner, ds_type=DatasetType.Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def custom_confusion_matrix(self, slice_size:int=1):\n",
    "        \"Confusion matrix as an `np.ndarray`.\"\n",
    "        x=torch.arange(0,self.data.c)\n",
    "        if slice_size is None: cm = ((self.pred_class==x[:,None]) & (self.y_true==x[:,None,None])).sum(2)\n",
    "        else:\n",
    "            cm = torch.zeros(self.data.c, self.data.c, dtype=x.dtype)\n",
    "            for i in range(0, self.y_true.shape[0], slice_size):\n",
    "                #cm_slice = ((self.pred_class[i:i+slice_size]==x[:,None])\n",
    "                            #& (self.y_true[i:i+slice_size]==x[:,None,None])).sum(2)\n",
    "                cm_slice = ((self.pred_class[i:i+slice_size]==x[:,None])\n",
    "                            & (self.y_true[i:i+slice_size]==(x[:,None,None]).float())).sum(2)\n",
    "                torch.add(cm, cm_slice, out=cm)\n",
    "        return to_np(cm)\n",
    "    \n",
    "fastai.train.ClassificationInterpretation.confusion_matrix = custom_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "interp.plot_top_losses(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLM Py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "293px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
