{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import sys\n",
    "###\n",
    "#!pip install openslide-python\n",
    "#https://github.com/deroneriksson/python-wsi-preprocessing/\n",
    "###\n",
    "sys.path.append('../python-wsi-preprocessing/')\n",
    "from deephistopath.wsi import slide, filter, tiles, util\n",
    "\n",
    "import fastai\n",
    "from fastai.vision import *\n",
    "from fastai.core import parallel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from functools import partial, update_wrapper\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from enum import Enum\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = 1000000000\n",
    "\n",
    "PATH = Path('/home/Deep_Learner/work/network/datasets/Hypophysenadenome/')\n",
    "FONT_PATH=PATH/'1984-Happines-Regular.ttf'\n",
    "ROIS_CORTICOTROP = PATH/'rois_corticotrop'\n",
    "ROIS_CORTICOTROP_FILTERED = PATH/'rois_corticotrop_filtered'\n",
    "ROIS_GONADOTROP = PATH/'rois_gonadotrop'\n",
    "ROIS_GONADOTROP_FILTERED = PATH/'rois_gonadotrop_filtered'\n",
    "TILES_CORTICOTROP = PATH/'tiles_corticotrop'\n",
    "TILES_GONADOTROP = PATH/'tiles_gonadotrop'\n",
    "\n",
    "ROIS_EXPERIMENTING = PATH/'rois_experimenting'\n",
    "ROIS_EXPERIMENTING_FILTERED = PATH/'rois_experimenting_filtered'\n",
    "TILES_EXPERIMENTING = PATH/'tiles_experimenting'\n",
    "\n",
    "PATH_RELAPSE = Path('/home/Deep_Learner/work/network/datasets/Hypophysenadenome-Rezidive/')\n",
    "RELAPSE_WSIS_EXPERIMENTING = PATH_RELAPSE/'wsis_experimenting'\n",
    "RELAPSE_PNGS_EXPERIMENTING = PATH_RELAPSE/'pngs_experimenting'\n",
    "RELAPSE_PNGS_FILTERED_EXPERIMENTING = PATH_RELAPSE/'pngs_filtered_experimenting'\n",
    "RELAPSE_TILES_EXPERIMENTING = PATH_RELAPSE/'tiles_experimenting'\n",
    "\n",
    "slide.BASE_DIR = PATH\n",
    "#slide.TRAIN_PREFIX = 'hypophysen'\n",
    "#slide.SRC_TRAIN_DIR = ROIS_CORTICOTROP\n",
    "slide.SRC_TRAIN_DIR = ROIS_GONADOTROP\n",
    "\n",
    "def show(np):\n",
    "    return util.np_to_pil(np)\n",
    "\n",
    "Path.ls = lambda x: [p for p in list(x.iterdir()) if '.ipynb_checkpoints' not in p.name]\n",
    "\n",
    "def show_multiple_images(path, rows = 3, figsize=(128, 64)):\n",
    "    imgs = [open_image(p) for p in path.ls()]\n",
    "    show_all(imgs=imgs, r=rows, figsize=figsize)\n",
    "    \n",
    "def show_multiple_images_big(path:pathlib.Path):\n",
    "    for p in path.ls():\n",
    "        plt.imshow(mpimg.imread(str(p)))\n",
    "        plt.show()\n",
    "        \n",
    "class AdenomaType(Enum):\n",
    "    experimenting = 0\n",
    "    corticotrop = 1\n",
    "    gonadotrop = 2\n",
    "    relapse_experimenting = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adenomaType = AdenomaType.relapse_experimenting\n",
    "\n",
    "if adenomaType == AdenomaType.gonadotrop:\n",
    "    rois_filtered_path = ROIS_GONADOTROP_FILTERED\n",
    "    rois_path = ROIS_GONADOTROP\n",
    "    tiles_path = TILES_GONADOTROP\n",
    "    \n",
    "if adenomaType == AdenomaType.corticotrop:\n",
    "    rois_filtered_path = ROIS_CORTICOTROP_FILTERED\n",
    "    rois_path = ROIS_CORTICOTROP\n",
    "    tiles_path = TILES_CORTICOTROP\n",
    "    \n",
    "if adenomaType == AdenomaType.experimenting:\n",
    "    rois_filtered_path = ROIS_EXPERIMENTING_FILTERED\n",
    "    rois_path = ROIS_EXPERIMENTING\n",
    "    tiles_path = TILES_EXPERIMENTING\n",
    "    \n",
    "if adenomaType == AdenomaType.relapse_experimenting:\n",
    "    wsi_path = RELAPSE_WSIS_EXPERIMENTING\n",
    "    rois_filtered_path = RELAPSE_PNGS_FILTERED_EXPERIMENTING\n",
    "    rois_path = RELAPSE_PNGS_EXPERIMENTING\n",
    "    tiles_path = RELAPSE_TILES_EXPERIMENTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overwrite parts of wsi lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide.SCALE_FACTOR = 1\n",
    "slide.BASE_DIR = PATH/'data'\n",
    "tiles.SUMMARY_TITLE_FONT_PATH = str(FONT_PATH)\n",
    "tiles.FONT_PATH = str(FONT_PATH)\n",
    "tiles.DISPLAY_TILE_SUMMARY_LABELS = True\n",
    "tiles.LABEL_ALL_TILES_IN_TOP_TILE_SUMMARY = True\n",
    "tiles.BORDER_ALL_TILES_IN_TOP_TILE_SUMMARY = True\n",
    "tiles.TISSUE_LOW_THRESH = 20\n",
    "\n",
    "##################################################################################################################\n",
    "\n",
    "slide.open_slide = slide.open_image\n",
    "\n",
    "##################################################################################################################\n",
    "\n",
    "def get_image_path(folder_path, slide_num):\n",
    "    return folder_path.ls()[slide_num]\n",
    "\n",
    "slide.get_filter_image_result = partial(get_image_path, rois_filtered_path)\n",
    "slide.get_training_image_path = partial(get_image_path, rois_path)\n",
    "slide.get_training_slide_path = partial(get_image_path, wsi_path)\n",
    "\n",
    "##################################################################################################################\n",
    "\n",
    "def parse_dimensions_from_image_filename(img_path):\n",
    "    shape = slide.open_image_np(img_path).shape\n",
    "    return shape[1], shape[0], shape[1], shape[0]\n",
    "\n",
    "slide.parse_dimensions_from_image_filename = parse_dimensions_from_image_filename\n",
    "\n",
    "##################################################################################################################\n",
    "\n",
    "def tile_to_pil_tile(tile):\n",
    "  \"\"\"\n",
    "  Convert tile information into the corresponding tile as a PIL image read from the whole-slide image file.\n",
    "\n",
    "  Args:\n",
    "    tile: Tile object.\n",
    "\n",
    "  Return:\n",
    "    Tile as a PIL image.\n",
    "  \"\"\"\n",
    "  t = tile\n",
    "  filepath = slide.get_training_image_path(t.slide_num)\n",
    "  img = slide.open_image_np(filepath)\n",
    "  #x, y = t.o_c_s, t.o_r_s\n",
    "  #w, h = t.o_c_e - t.o_c_s, t.o_r_e - t.o_r_s\n",
    "  tile = img[t.o_r_s:t.o_r_e, t.o_c_s:t.o_c_e,:]\n",
    "  tile_pil = util.np_to_pil(tile)\n",
    "  return tile_pil\n",
    "\n",
    "tiles.tile_to_pil_tile = tile_to_pil_tile\n",
    "\n",
    "##################################################################################################################\n",
    "def get_tile_image_path(tile):\n",
    "  \"\"\"\n",
    "  Obtain tile image path based on tile information such as row, column, row pixel position, column pixel position,\n",
    "  pixel width, and pixel height.\n",
    "\n",
    "  Args:\n",
    "    tile: Tile object.\n",
    "\n",
    "  Returns:\n",
    "    Path to image tile.\n",
    "  \"\"\"\n",
    "  t = tile\n",
    "  roi_name = slide.get_training_image_path(t.slide_num).stem\n",
    "  tile_path = os.path.join(PATH, tiles_path,\n",
    "                           roi_name + \"-\" + 'tile' + \"-r%d-c%d-x%d-y%d-w%d-h%d\" % (\n",
    "                             t.r, t.c, t.o_c_s, t.o_r_s, t.o_c_e - t.o_c_s, t.o_r_e - t.o_r_s) + \".\" + 'png')\n",
    "  return tile_path\n",
    "\n",
    "slide.get_tile_image_path = get_tile_image_path\n",
    "\n",
    "##################################################################################################################\n",
    "\n",
    "def top_tiles(self):\n",
    "    \"\"\"\n",
    "    Retrieve the top-scoring tiles.\n",
    "\n",
    "    Returns:\n",
    "       List of the top-scoring tiles.\n",
    "    \"\"\"\n",
    "    sorted_tiles = self.tiles_by_score()\n",
    "    top_tiles = [tile for tile in sorted_tiles\n",
    "                 if check_tile(tile)]\n",
    "    print(f'Number of top tiles/all tiles: {len(top_tiles)}/{len(sorted_tiles)}')\n",
    "    return top_tiles\n",
    "\n",
    "def check_tile(tile):\n",
    "    width = tile.o_c_e - tile.o_c_s\n",
    "    height = tile.o_r_e - tile.o_r_s\n",
    "    return tile.score > 0.55 and width >= 0.7*tiles.COL_TILE_SIZE and height >= 0.7*tiles.ROW_TILE_SIZE\n",
    "\n",
    "\n",
    "tiles.TileSummary.top_tiles = top_tiles\n",
    "\n",
    "##################################################################################################################\n",
    "from deephistopath.wsi.tiles import (hsv_saturation_and_value_factor, \n",
    "                                     hsv_purple_pink_factor, \n",
    "                                     tissue_quantity_factor, \n",
    "                                     tissue_quantity)\n",
    "\n",
    "def score_tile(np_tile, tissue_percent, slide_num, row, col):\n",
    "  \"\"\"\n",
    "  Score tile based on tissue percentage, color factor, saturation/value factor, and tissue quantity factor.\n",
    "\n",
    "  Args:\n",
    "    np_tile: Tile as NumPy array.\n",
    "    tissue_percent: The percentage of the tile judged to be tissue.\n",
    "    slide_num: Slide number.\n",
    "    row: Tile row.\n",
    "    col: Tile column.\n",
    "\n",
    "  Returns tuple consisting of score, color factor, saturation/value factor, and tissue quantity factor.\n",
    "  \"\"\"\n",
    "  color_factor = hsv_purple_pink_factor(np_tile)\n",
    "  s_and_v_factor = hsv_saturation_and_value_factor(np_tile)\n",
    "  amount = tissue_quantity(tissue_percent)\n",
    "  quantity_factor = tissue_quantity_factor(amount)\n",
    "  combined_factor = color_factor * s_and_v_factor\n",
    "  score = tissue_percent * combined_factor / 1000.0\n",
    "  # scale score to between 0 and 1\n",
    "  score = 1.0 - (10.0 / (10.0 + score))\n",
    "  #print(f'tp: {tissue_percent}')\n",
    "  #print(f'cf: {combined_factor}')\n",
    "  #print(f'score: {score}')  \n",
    "  return score, color_factor, s_and_v_factor, quantity_factor\n",
    "\n",
    "tiles.score_tile = score_tile\n",
    "#tiles.score_tile = tiles.score_tile\n",
    "#############################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert WSIs into .png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/Deep_Learner/work/network/datasets/Hypophysenadenome-Rezidive/wsis_experimenting/1579-12-III-HE.ndpi')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slide.get_training_slide_path(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening Slide #0: /home/Deep_Learner/work/network/datasets/Hypophysenadenome-Rezidive/wsis_experimenting/1579-12-III-HE.ndpi\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TiffImageFile' object has no attribute 'dimensions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c21f8f928a8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mslide\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_slide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/network/python-wsi-preprocessing/deephistopath/wsi/slide.py\u001b[0m in \u001b[0;36mshow_slide\u001b[0;34m(slide_number)\u001b[0m\n\u001b[1;32m    713\u001b[0m     \u001b[0mslide_number\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mslide\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m   \"\"\"\n\u001b[0;32m--> 715\u001b[0;31m   \u001b[0mpil_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslide_to_scaled_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslide_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m   \u001b[0mpil_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/network/python-wsi-preprocessing/deephistopath/wsi/slide.py\u001b[0m in \u001b[0;36mslide_to_scaled_pil_image\u001b[0;34m(slide_number)\u001b[0m\n\u001b[1;32m    681\u001b[0m   \u001b[0mslide\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_slide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslide_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m   \u001b[0mlarge_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlarge_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslide\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m   \u001b[0mnew_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlarge_w\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mSCALE_FACTOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m   \u001b[0mnew_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlarge_h\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mSCALE_FACTOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TiffImageFile' object has no attribute 'dimensions'"
     ]
    }
   ],
   "source": [
    "slide.show_slide(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# for 'normal' img formats like .png\n",
    "##\n",
    "\n",
    "def filter_roi(img_path:pathlib.Path, index:int):\n",
    "    if img_path.suffix == '.png':\n",
    "        try:\n",
    "            img_pil = slide.open_image(img_path)\n",
    "            img_np = util.pil_to_np_rgb(img_pil)\n",
    "            grayscale_np = filter.filter_rgb_to_grayscale(img_np)\n",
    "            complement_np = filter.filter_complement(grayscale_np)\n",
    "            otsu_np = filter.filter_otsu_threshold(complement_np).astype(np.bool)\n",
    "            filtered_img_np = util.mask_rgb(img_np, otsu_np)\n",
    "            filtered_img_pil = util.np_to_pil(filtered_img_np)\n",
    "            #filtered_path = rois_filtered_path/f'{img_path.stem}-filtered{img_path.suffix}'\n",
    "            filtered_path = rois_filtered_path/f'{img_path.stem}{img_path.suffix}'\n",
    "            filtered_img_pil.save(filtered_path)\n",
    "        except:\n",
    "            print(img_path)\n",
    "            \n",
    "failed = []\n",
    "try:\n",
    "    parallel(filter_roi, rois_path.ls(), max_workers=25)\n",
    "except:\n",
    "    failed.append(img_path)\n",
    "    \n",
    "print(failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multiple_images_big(rois_filtered_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multi process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles.multiprocess_filtered_images_to_tiles(display=False, \n",
    "                                            save_summary=False, \n",
    "                                            save_data=False, \n",
    "                                            save_top_tiles=True,\n",
    "                                            html=False, \n",
    "                                            image_num_list=list(range(0, len(rois_filtered_path.ls()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tiles_path.ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multiple_images_big(tiles_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -r {rois_filtered_path/'*'}\n",
    "#!rm -r {tiles_path/'*'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -r {tiles_path/'*'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = []\n",
    "for n, p in tqdm(enumerate(rois_filtered_path.ls()[:1]), total=len(rois_filtered_path.ls())-1):\n",
    "    if p.suffix == '.png':\n",
    "        try:\n",
    "            print(p)\n",
    "            tiles.summary_and_tiles(n, display=False, save_summary=False, save_data=False, save_top_tiles=True)\n",
    "        except:\n",
    "            failed.append(p)\n",
    "            \n",
    "print(failed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "p = '/home/Deep_Learner/work/network/datasets/Hypophysenadenome/rois_corticotrop/1000-13-III-HE-ROI_1-ACTH.png'\n",
    "\n",
    "img_pil = slide.open_image(p)\n",
    "img_np = util.pil_to_np_rgb(img_pil)\n",
    "grayscale_np = filter.filter_rgb_to_grayscale(img_np)\n",
    "complement_np = filter.filter_complement(grayscale_np)\n",
    "otsu_np = filter.filter_otsu_threshold(complement_np).astype(np.bool)\n",
    "filtered_img_np = util.mask_rgb(img_np, otsu_np)\n",
    "\n",
    "plt.imshow(filtered_img_np)\n",
    "\n",
    "tiles = []\n",
    "sz = 512\n",
    "for i in range(int(filtered_img_np.shape[0]/sz)):\n",
    "    for j in range(int(filtered_img_np.shape[1]/sz)):\n",
    "         tiles.append(filtered_img_np[i*sz:(i+1)*sz,j*sz:(j+1)*sz])\n",
    "        \n",
    "\n",
    "for t in tiles:\n",
    "    print(t.shape)\n",
    "    nz = np.count_nonzero(t)// 3\n",
    "    print(f'tissue percentage:{(nz/(t.shape[0]*t.shape[1]))*100}')\n",
    "    plt.imshow(t)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
