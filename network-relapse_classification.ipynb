{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "#device = 0\n",
    "#torch.cuda.set_device(device)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('../fastai/')\n",
    "import fastai\n",
    "from fastai.vision import *\n",
    "from fastai.vision.learner import model_meta\n",
    "\n",
    "sys.path.append('../models-pytorch/pretrained-models.pytorch')\n",
    "import pretrainedmodels\n",
    "from pretrainedmodels import *\n",
    "\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models import *\n",
    "from torchsummary import summary\n",
    "from pathlib import Path\n",
    "from functools import partial, update_wrapper\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.image as mpimg\n",
    "import shutil\n",
    "\n",
    "\n",
    "PATH = Path('/home/Deep_Learner/private/network/datasets/Hypophysenadenome-Rezidive/')\n",
    "PATH_LOCAL = Path('/home/Deep_Learner/private/local/')\n",
    "WSIS_RELAPSE = PATH/'wsis_relapse'\n",
    "TILES_RELAPSE = PATH/'tiles_relapse'\n",
    "TILES_NON_RELAPSE = PATH/'tiles_non_relapse'\n",
    "\n",
    "LABELS_NAME = 'rezidive-labels.xlsx'\n",
    "LABELS = PATH/LABELS_NAME\n",
    "\n",
    "\n",
    "\n",
    "nw = 16   #number of workers for data loader\n",
    "torch.backends.cudnn.benchmark=True\n",
    "\n",
    "#def batch_stats(self, funcs:Collection[Callable]=None)->Tensor:\n",
    "#        \"Grab a batch of data and call reduction function `func` per channel\"\n",
    "#        funcs = ifnone(funcs, [torch.mean,torch.std])\n",
    "#        x = self.one_batch(ds_type=DatasetType.Train, denorm=False)[0].cpu()\n",
    "#        return [func(channel_view(x), 1) for func in funcs]\n",
    "#        \n",
    "#vision.data.ImageDataBunch.batch_stats = batch_stats\n",
    "\n",
    "sz = 512\n",
    "bs = 6\n",
    "\n",
    "#fastai defaults\n",
    "tta_beta = 0.4 \n",
    "tta_scale = 1.35\n",
    "dropout = 0.5\n",
    "wd = 0.01\n",
    "\n",
    "seed = 23\n",
    "np.random.seed(seed)\n",
    "\n",
    "num2lbs = {\n",
    "    0:\"non_relapse\", \n",
    "    1:\"relapse\"\n",
    "}\n",
    "\n",
    "lbs2num = {l:n for n,l in num2lbs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.torch_core import flatten_model\n",
    "\n",
    "def arch_summary(arch):\n",
    "    model = arch(False)\n",
    "    tot = 0\n",
    "    for i, l in enumerate(model.children()):\n",
    "        n_layers = len(flatten_model(l))\n",
    "        tot += n_layers\n",
    "        print(f'({i}) {l.__class__.__name__:<12}: {n_layers:<4}layers (total: {tot})')\n",
    "\n",
    "def show(np):\n",
    "    return util.np_to_pil(np)\n",
    "\n",
    "Path.ls = lambda x: [p for p in list(x.iterdir()) if '.ipynb_checkpoints' not in p.name]\n",
    "\n",
    "def show_multiple_images(path, rows = 3, figsize=(128, 64)):\n",
    "    imgs = [open_image(p) for p in path.ls()]\n",
    "    show_all(imgs=imgs, r=rows, figsize=figsize)\n",
    "    \n",
    "def show_multiple_images_big(path:pathlib.Path):\n",
    "    for p in path.ls():\n",
    "        plt.imshow(mpimg.imread(str(p)))\n",
    "        plt.show()\n",
    "        \n",
    "def get_id_from_path(path):\n",
    "    path = Path(path)\n",
    "    split = path.stem.split('-')\n",
    "    return f'{split[0]}-{split[1]}'\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    result = []\n",
    "    for l in list_of_lists:\n",
    "        if len(l) == 1:\n",
    "            result.append(l[0])\n",
    "        else:\n",
    "            for elem in l:\n",
    "                result.append(elem)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/PPPW/deep-learning-random-explore/blob/master/CNN_archs/cnn_archs.ipynb\n",
    "\n",
    "def identity(x): return x\n",
    "\n",
    "def nasnetamobile(pretrained=True):\n",
    "    pretrained = 'imagenet' if pretrained else None\n",
    "    model = pretrainedmodels.nasnetamobile(pretrained=pretrained, num_classes=1000)  \n",
    "    model.logits = identity\n",
    "    model_meta[nasnetamobile] =  { 'cut': identity, 'split': lambda m: (list(m[0][0].children())[8], m[1]) }\n",
    "    return nn.Sequential(model)\n",
    "\n",
    "#arch_summary(lambda _: nasnetamobile(False)[0])\n",
    "\n",
    "def se_resnext50_32x4d(pretrained=True):\n",
    "    pretrained = 'imagenet' if pretrained else None\n",
    "    model = pretrainedmodels.se_resnext50_32x4d(pretrained=pretrained)\n",
    "    model_meta[se_resnext50_32x4d] =  {'cut': -2, 'split': lambda m: (m[0][3], m[1]) }\n",
    "    return model\n",
    "\n",
    "#arch_summary(lambda _: pretrainedmodels.se_resnext50_32x4d(pretrained=None))\n",
    "\n",
    "def se_resnext101_32x4d(pretrained=True):\n",
    "    pretrained = 'imagenet' if pretrained else None\n",
    "    model = pretrainedmodels.se_resnext101_32x4d(pretrained=pretrained)\n",
    "    model_meta[se_resnext101_32x4d] =  {'cut': -2, 'split': lambda m: (m[0][3], m[1]) }\n",
    "    return model\n",
    "\n",
    "def xception(pretrained=True):\n",
    "    pretrained = 'imagenet' if pretrained else None\n",
    "    model = pretrainedmodels.xception(pretrained=pretrained)\n",
    "    model_meta[xception] =  { 'cut': -1, 'split': lambda m: (m[0][11], m[1]) }\n",
    "    return model\n",
    "\n",
    "def inceptionv4(pretrained=True):\n",
    "    pretrained = 'imagenet' if pretrained else None\n",
    "    model = pretrainedmodels.inceptionv4(pretrained=pretrained)\n",
    "    model_meta[xception] =  { 'cut': -2, 'split': lambda m: (m[0][11], m[1]) }\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#n='test'\n",
    "\n",
    "n = np.load('n-rez.npy')\n",
    "print(n)\n",
    "\n",
    "m = n+1\n",
    "m=1\n",
    "np.save('n-rez', m)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split dataset into train, valid and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b873d35cfbce49a8afa97d22b6947963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24596), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "seed: 23\n",
      "49231\n",
      "8139\n",
      "831\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(LABELS).set_index('id')\n",
    "test_pct = 0.000000001\n",
    "valid_pct = 0.1\n",
    "\n",
    "###\n",
    "# RELAPSE\n",
    "###\n",
    "\n",
    "#key: patient, value: list of wsi names\n",
    "patient_to_wsi_ids_relapse = {}\n",
    "ids_relapse_all = [get_id_from_path(p) for p in (WSIS_RELAPSE.ls()) if p.suffix == '.ndpi']\n",
    "excluded_ids = ['1883-13', '1175-12']\n",
    "for id in ids_relapse_all:\n",
    "    if id not in excluded_ids:\n",
    "        patient = df.at[id, 'Patient']\n",
    "        if patient in patient_to_wsi_ids_relapse.keys():\n",
    "            patient_to_wsi_ids_relapse[patient].append(id)\n",
    "        else:\n",
    "            patient_to_wsi_ids_relapse[patient] = [id]\n",
    "            \n",
    "patients_relapse_train_and_valid, patients_relapse_test = train_test_split(list(patient_to_wsi_ids_relapse.keys()), test_size=test_pct, random_state=seed)\n",
    "patients_relapse_train, patients_relapse_valid = train_test_split(patients_relapse_train_and_valid, test_size=valid_pct, random_state=seed)\n",
    "\n",
    "ids_relapse_train = flatten([patient_to_wsi_ids_relapse[pat] for pat in patients_relapse_train])\n",
    "ids_relapse_valid = flatten([patient_to_wsi_ids_relapse[pat] for pat in patients_relapse_valid])\n",
    "ids_relapse_test = flatten([patient_to_wsi_ids_relapse[pat] for pat in patients_relapse_test])\n",
    "\n",
    "tile_paths_relapse_all = [Path(f'{p.parts[-2]}/{p.parts[-1]}') for p in (TILES_RELAPSE.ls()) if p.suffix == '.png']\n",
    "tile_paths_relapse_train = [p for p in tile_paths_relapse_all if get_id_from_path(p) in ids_relapse_train]\n",
    "tile_paths_relapse_val = [p for p in tile_paths_relapse_all if get_id_from_path(p) in ids_relapse_valid]\n",
    "tile_paths_relapse_test = [p for p in tile_paths_relapse_all if get_id_from_path(p) in ids_relapse_test]\n",
    "\n",
    "\n",
    "###\n",
    "# NON RELAPSE\n",
    "###\n",
    "tile_paths_non_relapse_all = [Path(f'{p.parts[-2]}/{p.parts[-1]}') for p in (TILES_NON_RELAPSE.ls()) if p.suffix == '.png']\n",
    "\n",
    "ids_non_relapse_all = []\n",
    "for p in tqdm(tile_paths_non_relapse_all):\n",
    "    ids_non_relapse_all.append(get_id_from_path(p))\n",
    "ids_non_relapse_all = list(set(ids_non_relapse_all))\n",
    "\n",
    "\n",
    "ids_non_relapse_train_and_valid, ids_non_relapse_test = train_test_split(ids_non_relapse_all, test_size=test_pct, random_state=seed)\n",
    "ids_non_relapse_train, ids_non_relapse_val = train_test_split(ids_non_relapse_train_and_valid, test_size=valid_pct, random_state=seed)\n",
    "\n",
    "tile_paths_non_relapse_train = [p for p in tile_paths_non_relapse_all if get_id_from_path(p) in ids_non_relapse_train]\n",
    "tile_paths_non_relapse_val = [p for p in tile_paths_non_relapse_all if get_id_from_path(p) in ids_non_relapse_val]\n",
    "tile_paths_non_relapse_test = [p for p in tile_paths_non_relapse_all if get_id_from_path(p) in ids_non_relapse_test]\n",
    "\n",
    "###\n",
    "# COMBINE\n",
    "###\n",
    "tile_paths_train = tile_paths_non_relapse_train + tile_paths_relapse_train\n",
    "tile_paths_val = tile_paths_non_relapse_val + tile_paths_relapse_val\n",
    "tile_paths_test = tile_paths_non_relapse_test + tile_paths_relapse_test\n",
    "\n",
    "df_tile_paths_train_and_valid = pd.DataFrame((tile_paths_train+tile_paths_val), columns=['name'])\n",
    "\n",
    "print(f'seed: {seed}')\n",
    "print(len(tile_paths_train))\n",
    "print(len(tile_paths_val))\n",
    "print(len(tile_paths_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = get_transforms(flip_vert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfms = ([RandTransform(tfm=TfmAffine (dihedral_affine), kwargs={}, p=1.0, resolved={}, do_run=True, is_random=True),\n",
    "#        RandTransform(tfm=TfmLighting (brightness), kwargs={'change': (0.475, 0.525)}, p=0.75, resolved={}, do_run=True, is_random=True),\n",
    "#        RandTransform(tfm=TfmLighting (contrast), kwargs={'scale': (0.95, 1.0526315789473684)}, p=0.75, resolved={}, do_run=True, is_random=True)],\n",
    "#        [])\n",
    "\n",
    "#def get_ex(): return open_image(str(TRAIN.ls()[0]))\n",
    "#\n",
    "#def plots_f(rows, cols, width, height, **kwargs):\n",
    "#    [get_ex().apply_tfms(tfms[0], **kwargs).show(ax=ax) for i,ax in enumerate(plt.subplots(\n",
    "#        rows,cols,figsize=(width,height))[1].flatten())]\n",
    "#\n",
    "#plots_f(2, 4, 12, 6, size=224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datablock API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_func(path):\n",
    "    path = Path(path)\n",
    "    id = get_id_from_path(path)\n",
    "    if id in ids_non_relapse_all:\n",
    "        return [lbs2num['non_relapse']]\n",
    "    else:\n",
    "        return [lbs2num['relapse']]\n",
    "\n",
    "    \n",
    "def split_func(path):\n",
    "    path = Path(path)\n",
    "    return get_id_from_path(path) in (ids_non_relapse_val+ids_relapse_valid) \n",
    "\n",
    "#data = ImageList.from_folder(path=TRAIN, extensions=['.png'])\n",
    "data = ImageList.from_df(df_tile_paths_train_and_valid, path=PATH)\n",
    "data = data.split_by_valid_func(split_func)\n",
    "data = data.label_from_func(label_func)\n",
    "data = data.transform(tfms=tfms, size=sz)\n",
    "#data = data.add_test_folder(test_folder=TEST_EXPERIMENTING)\n",
    "data = data.add_test([PATH/p for p in tile_paths_test])\n",
    "data = data.databunch(bs=bs, num_workers=nw, path=PATH)\n",
    "data = data.normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_frozen = 5\n",
    "epochs_unfrozen = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = resnext101_32x8d\n",
    "learner = cnn_learner(data=data, \n",
    "                     base_arch=arch, \n",
    "                     metrics=[accuracy_thresh], \n",
    "                     ps=dropout, \n",
    "                     pretrained=True, \n",
    "                     wd = wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1-resnext101_32x8d-size512-bs6-epochs_head5-epochs_complete10-seed_23'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nameBase = f'{n}-{arch.__name__}-size{sz}-bs{bs}-epochs_head{epochs_frozen}-epochs_complete{epochs_unfrozen}-seed_{seed}'\n",
    "nameBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.lr_find()\n",
    "learner.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(cyc_len=epochs_frozen, max_lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameHead = f'{nameBase}-head'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save(nameHead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner.load(nameHead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find()\n",
    "learner.recorder.plot(skip_start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = 2e-7\n",
    "lr3 = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(cyc_len=epochs_unfrozen, max_lr=slice(lr2, lr3), callbacks=[SaveModelCallback(learner, every='epoch', monitor='accuracy_thresh')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.recorder.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameComplete = f'{nameBase}-complete'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save(nameComplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner.load(nameComplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner.load(data.path/'models/6-resnext101_32x8d-size512-bs8-seed_73/bestmodel_15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('bestmodel_6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### best learning schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction per case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(predicted_classes:list, all_classes:list):\n",
    "    for c in predicted_classes:\n",
    "        assert c in all_classes\n",
    "    n = len(all_classes)\n",
    "    res = np.zeros(n, int)\n",
    "    for i, c in enumerate(all_classes):\n",
    "        if c in predicted_classes:\n",
    "            res[i] = 1 \n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def ensemble_predict(dict_arch_to_path_of_saved_model:typing.Dict[Callable, pathlib.Path], \n",
    "                     data:fastai.vision.data.ImageDataBunch,\n",
    "                     ds_type:fastai.basic_data.DatasetType,\n",
    "                     tta:bool, \n",
    "                     scale:float,\n",
    "                     beta:float):\n",
    "    \"\"\"\n",
    "    tta: Should test time augmentation be used?\n",
    "    scale: if tta is True -> scaling factor for tta\n",
    "    beta: if tta is True -> beta factor for tta\n",
    "    check this out for more infos: https://docs.fast.ai/basic_train.html#Test-time-augmentation\n",
    "    \"\"\"\n",
    "   \n",
    "    print(f'{str([a.__name__ for a in dict_arch_to_path_of_saved_model.keys()])}_sz{sz}_ensembled')\n",
    "    \n",
    "    predsList = []\n",
    "    for arch in dict_arch_to_path_of_saved_model.keys():\n",
    "        learner = cnn_learner(data=data, base_arch=arch, pretrained=False)\n",
    "        learner.load(dict_arch_to_path_of_saved_model[arch])\n",
    "        if tta is True:\n",
    "            preds = learner.TTA(beta=beta, scale=scale, ds_type=ds_type)\n",
    "        else:\n",
    "            preds = learner.get_preds(ds_type=ds_type)\n",
    "            \n",
    "        predsList.append(preds)\n",
    "    \n",
    "    preds_ensembled = predsList[0]\n",
    "    for n, _ in enumerate(predsList):\n",
    "        if n == 0:\n",
    "            continue\n",
    "        else:\n",
    "            preds_ensembled[0] = preds_ensembled[0] + predsList[n][0]\n",
    "    preds_ensembled[0] = preds_ensembled[0]/len(predsList)\n",
    "    \n",
    "    return preds_ensembled\n",
    "\n",
    "def from_preds_to_dict_path_to_preds(preds, \n",
    "                                     imageDataBunch:fastai.vision.ImageDataBunch, \n",
    "                                     ds_type:fastai.basic_data.DatasetType,\n",
    "                                     threshold:float):\n",
    "    \"\"\"\n",
    "    preds: What fastai.vision.learner.get_preds or fastai.vision.learner.TTA return.\n",
    "            two tensors: 1st: lists with raw predictions for each class of an image\n",
    "                         2nd: lists with y_true\n",
    "            form e.g. [tensor([[0.9672, 0.9211, 0.4560, 0.8185], \n",
    "                                [0.9498, 0.8600, 0.5852, 0.7206]]),\n",
    "                         tensor([[0., 0., 0., 1.],\n",
    "                                [0., 0., 1., 1.]])]\n",
    "                                \n",
    "    RETURN:\n",
    "        key:path, value:tuple (fastai.core.MultiCategory, tensor preds one hot encoded, tensor with pure preds) \n",
    "        e.g. (MultiCategory 0, tensor([1., 0., 0., 0.]), tensor([0.9952, 0.0015, 0.0021, 0.0029]))\n",
    "    path_to_pred = {}\n",
    "    \"\"\"\n",
    "    #key:path, value:tuple (fastai.core.MultiCategory, tensor preds one hot encoded, tensor with pure preds) \n",
    "    #e.g. (MultiCategory 0, tensor([1., 0., 0., 0.]), tensor([0.9952, 0.0015, 0.0021, 0.0029]))\n",
    "    path_to_pred = {}\n",
    "    d = None\n",
    "    if ds_type is DatasetType.Valid:\n",
    "        d = imageDataBunch.valid_ds\n",
    "    elif ds_type is DatasetType.Test:\n",
    "        d = imageDataBunch.test_ds\n",
    "    elif ds_type is DatasetType.Train:\n",
    "        d = imageDataBunch.train_ds\n",
    "    for path, pred in tqdm(zip(d.items, preds[0]), total = len(d.items)):\n",
    "        multi_c = None\n",
    "        pred_one_hot_encoded = (pred > threshold).float()\n",
    "        pred_raw = pred\n",
    "        path_to_pred[path] = multi_c, pred_one_hot_encoded, pred_raw\n",
    "        \n",
    "    return path_to_pred\n",
    "\n",
    "\n",
    "def get_class_occurence_per_id(learner:fastai.vision.learner=None,\n",
    "                               labelList:fastai.data_block.LabelList=None,\n",
    "                               dict_arch_to_path_of_saved_model:typing.Dict[Callable, pathlib.Path]=None,\n",
    "                               imageDataBunch:fastai.vision.data.ImageDataBunch=None,\n",
    "                               ds_type:fastai.basic_data.DatasetType=None,\n",
    "                               tta:bool=False,                                          \n",
    "                               threshold = 0.5,                              \n",
    "                               scale:float = 1.35,\n",
    "                               beta: float = 0.4):\n",
    "    \"\"\"\n",
    "    Option 1: Hand over a fastai.vision.learner and fastai.data_block.LabelList. No tta and no ensembling available\n",
    "                for this option.\n",
    "    Option 2: Hand over a fastai.vision.learner that was initalized with a fastai.vision.data.ImageDataBunch object.\n",
    "    Option 3: Hand over dict where the keys are functions to create a model (e.g. torchvision.models.resnet50)\n",
    "                and the values are paths to saved weights. Do this to use ensembling.\n",
    "    \n",
    "    Params:\n",
    "        threshold:  threshold to consider the predictions to be correct or not\n",
    "        scale: only needed when tta is True; scale value for fastai's fastai.basic_train.Learner.TTA function\n",
    "        beta: only needed when tta is True; beta value for fastai's fastai.basic_train.Learner.TTA function\n",
    "    \"\"\"\n",
    "    \n",
    "    if labelList is not None and ds_type is not None:\n",
    "        raise ValueError('One of dataset or ds_type must be None')\n",
    "    if labelList is not None and tta is True:\n",
    "        raise ValueError('TTA is not available for a custom LabelList')\n",
    "                \n",
    "    #key:path, value:tuple (fastai.core.MultiCategory, tensor preds one hot encoded, tensor with pure preds) \n",
    "    #e.g. (MultiCategory 0, tensor([1., 0., 0., 0.]), tensor([0.9952, 0.0015, 0.0021, 0.0029]))\n",
    "    path_to_pred = {}\n",
    "    \n",
    "    #Option 1\n",
    "    if learner is not None and labelList is not None:\n",
    "        for n, path in tqdm(enumerate(labelList.items), total=len(labelList.items)):\n",
    "            pred = learner.predict(labelList[n][0], thresh=threshold)\n",
    "            path_to_pred[path] = pred\n",
    "    \n",
    "    #Option 2\n",
    "    elif learner is not None and labelList is None and  not dict_arch_to_path_of_saved_model and imageDataBunch is None:\n",
    "        if tta is True:\n",
    "            preds = learner.TTA(beta=beta, scale=scale, ds_type=ds_type)\n",
    "        else:\n",
    "            preds = learner.get_preds(ds_type=ds_type)\n",
    "        path_to_pred = from_preds_to_dict_path_to_preds(preds, learner.data, ds_type, threshold)\n",
    "                \n",
    "    #Option 3\n",
    "    elif dict_arch_to_path_of_saved_model and imageDataBunch is not None:\n",
    "        preds = ensemble_predict(dict_arch_to_path_of_saved_model, imageDataBunch, ds_type, tta, scale, beta)\n",
    "        path_to_pred = from_preds_to_dict_path_to_preds(preds, imageDataBunch, ds_type, threshold)                \n",
    "               \n",
    "    #key: id of a case; value: list with this syntax  \n",
    "    #[<number of tiles>, \n",
    "    #[<number of occurence of class1 over all tiles per id>, \n",
    "    #<number of occurence of class2 over all tiles per id>, ..., \n",
    "    #<number of occurence of classN over all tiles per id>],\n",
    "    #y_true]\n",
    "    class_occurence_per_id = {}\n",
    "    \n",
    "    for path, pred in path_to_pred.items():   \n",
    "        id = get_id_from_path(path)\n",
    "        if id in class_occurence_per_id:\n",
    "            v = class_occurence_per_id[id]\n",
    "            v[0] = v[0] + 1\n",
    "            v[1] = v[1] + pred[1]\n",
    "            class_occurence_per_id[id] = v\n",
    "        else:\n",
    "            class_occurence_per_id[id] = [1, pred[1], one_hot_encode(label_func(path), lbs2num.values())]\n",
    "            \n",
    "    return class_occurence_per_id\n",
    "\n",
    "\n",
    "def get_preds_threshold_per_id(thresholds_per_class:list, class_occurence_per_id:dict):\n",
    "    #key: id of a case; \n",
    "    #value: list with this syntax  \n",
    "    #[y_pred_th e.g. [True,False,False,False], \n",
    "    #y_true e.g. [1,0,0,0]]\n",
    "    result = {}\n",
    "    for k in class_occurence_per_id.keys():\n",
    "        y_pred_th = []\n",
    "        for n, i in enumerate(class_occurence_per_id[k][1]):\n",
    "            i = int(i)\n",
    "            y_pred_th.append(i/class_occurence_per_id[k][0] > thresholds_per_class[n])\n",
    "    \n",
    "        result[k] = [y_pred_th, class_occurence_per_id[k][2]]\n",
    "    return result\n",
    "\n",
    "def get_accuracy_over_all_ids(number_of_ids, preds_threshold_per_id:dict, per_class:bool = True, number_of_classes = len(lbs2num)):\n",
    "    if per_class is True:\n",
    "        correctly_predicted = np.zeros(number_of_classes, dtype=np.int)\n",
    "    else:\n",
    "        correctly_predicted = 0\n",
    "    for k in preds_threshold_per_id.keys():\n",
    "        pred = preds_threshold_per_id[k][0]\n",
    "        true = preds_threshold_per_id[k][1]\n",
    "        for i in range(number_of_classes):\n",
    "            if true[i] == pred[i]:\n",
    "                if per_class is True:\n",
    "                    correctly_predicted[i] = correctly_predicted[i] + 1\n",
    "                else:\n",
    "                    correctly_predicted = correctly_predicted + 1\n",
    "    if per_class is True:                    \n",
    "        correctly_predicted_percentage = {}\n",
    "        for lb, num in zip(lbs2num.keys(), correctly_predicted):\n",
    "            correctly_predicted_percentage[lb] = num/number_of_ids\n",
    "    if per_class is False:\n",
    "        correctly_predicted_percentage = correctly_predicted/number_of_ids\n",
    "\n",
    "    return correctly_predicted_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arches = {resnext101_32x8d:Path(MODEL_PATH/'6-resnext101_32x8d-size512-bs8-seed_73/bestmodel_15'),\n",
    "          se_resnext101_32x4d:MODEL_PATH/'11-se_resnext101_32x4d-size512-bs10-epochs_head5-epochs_complete5-seed_73/11-se_resnext101_32x4d-size512-bs8-epochs_head5-epochs_complete5-seed_73-complete'}\n",
    "ths = [0.5,0.5,0.5,0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copi_val = get_class_occurence_per_id(dict_arch_to_path_of_saved_model=arches,\n",
    "#                                      imageDataBunch=data,\n",
    "#                                      ds_type=DatasetType.Valid)\n",
    "copi_val = get_class_occurence_per_id(learner=learner, ds_type=DatasetType.Valid, tta=True)\n",
    "preds_th_val = get_preds_threshold_per_id(ths, copi_val)\n",
    "accuracy_per_class_val = get_accuracy_over_all_ids(len(preds_th_val), preds_th_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_per_class_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seed 73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copi_test = get_class_occurence_per_id(dict_arch_to_path_of_saved_model=arches,\n",
    "#                                      imageDataBunch=data,\n",
    "#                                      ds_type=DatasetType.Test)\n",
    "copi_test = get_class_occurence_per_id(learner=learner, ds_type=DatasetType.Test, tta=True)\n",
    "preds_th_test = get_preds_threshold_per_id(ths, copi_test)\n",
    "accuracy_per_class_test = get_accuracy_over_all_ids(len(preds_th_test), preds_th_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_per_class_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seed 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def custom_confusion_matrix(self, slice_size:int=1):\n",
    "        \"Confusion matrix as an `np.ndarray`.\"\n",
    "        x=torch.arange(0,self.data.c)\n",
    "        if slice_size is None: cm = ((self.pred_class==x[:,None]) & (self.y_true==x[:,None,None])).sum(2)\n",
    "        else:\n",
    "            cm = torch.zeros(self.data.c, self.data.c, dtype=x.dtype)\n",
    "            for i in range(0, self.y_true.shape[0], slice_size):\n",
    "                #cm_slice = ((self.pred_class[i:i+slice_size]==x[:,None])\n",
    "                            #& (self.y_true[i:i+slice_size]==x[:,None,None])).sum(2)\n",
    "                cm_slice = ((self.pred_class[i:i+slice_size]==x[:,None])\n",
    "                            & (self.y_true[i:i+slice_size]==(x[:,None,None]).float())).sum(2)\n",
    "                torch.add(cm, cm_slice, out=cm)\n",
    "        return to_np(cm)\n",
    "    \n",
    "fastai.train.ClassificationInterpretation.confusion_matrix = custom_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "interp.plot_top_losses(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds,y=learner.TTA(ds_type=DatasetType.Valid, scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pred_score_tta_1=auc_score_1(preds,y)\n",
    "pred_score_tta_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pred_score_tta_2=auc_score_2(preds,y)\n",
    "pred_score_tta_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## ROC curve and AUC on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds, roc_auc = roc_curve_custom(preds, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlim([-0.01, 1.0])\n",
    "plt.ylim([0.0, 1.01])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Finding threshold on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pred = preds[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pred_tensor = pred\n",
    "y_tensor = y\n",
    "\n",
    "pred = np.asarray(pred)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_np(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def F1_soft(preds,targs,th=0.,d=25.0):\n",
    "    preds = sigmoid_np(d*(preds - th))\n",
    "    targs = targs.astype(np.float)\n",
    "    score = 2.0*(preds*targs).sum(axis=0)/((preds+targs).sum(axis=0) + 1e-6)\n",
    "    return score\n",
    "\n",
    "def fit_val(x,y):\n",
    "    params = np.zeros(1)\n",
    "    wd = 1e-5\n",
    "    error = lambda p: np.concatenate((F1_soft(x,y,p) - 1.0,\n",
    "                                      wd*p), axis=None)\n",
    "    p, success = opt.leastsq(error, params)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "th = fit_val(pred, y)\n",
    "print('Thresholds: ',th)\n",
    "print('F1 macro: ', sklearn.metrics.f1_score(y, pred>th, average='macro'))\n",
    "print('F1 macro (th = 0.0): ', sklearn.metrics.f1_score(y, pred>0.0, average='macro'))\n",
    "print('F1 micro: ', sklearn.metrics.f1_score(y, pred>th, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "th, score, cv = 0,0,10\n",
    "for i in range(cv):\n",
    "    xt,xv,yt,yv = train_test_split(pred,y,test_size=0.5,random_state=i)\n",
    "    th_i = fit_val(xt,yt)\n",
    "    th += th_i\n",
    "    score +=  sklearn.metrics.f1_score(yv, xv>th_i, average='macro')\n",
    "th/=cv\n",
    "score/=cv\n",
    "print('Thresholds: ',th)\n",
    "print('F1 macro avr:',score)\n",
    "print('F1 macro: ', sklearn.metrics.f1_score(y, pred>th, average='macro'))\n",
    "print('F1 micro: ', sklearn.metrics.f1_score(y, pred>th, average='micro'))\n",
    "\n",
    "\n",
    "print('Fractions: ',(pred > th).mean(axis=0))\n",
    "print('Fractions (true): ',(y > 0.5).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f1 =  sklearn.metrics.f1_score(y, pred>th, average=None)\n",
    "bins = np.linspace(pred[:].min(), pred[:].max(), 50)\n",
    "plt.hist(pred[y[:] == 0][:], bins, alpha=0.5, log=True, label='false')\n",
    "plt.hist(pred[y[:] == 1][:], bins, alpha=0.5, log=True, label='true')\n",
    "plt.legend(loc='upper right')\n",
    "plt.axvline(x=th[0], color='k', linestyle='--')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLM Py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "293px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
